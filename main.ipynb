{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.2 64-bit",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_addons.utils.types import FloatTensorLike\n",
    "\n",
    "from typing import Union, Callable, Dict\n",
    "from typeguard import typechecked\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Addons\")\n",
    "class RadaBelief(tf.keras.optimizers.Optimizer):\n",
    "\n",
    "    @typechecked\n",
    "    def __init__(\n",
    "            self,\n",
    "            learning_rate: Union[FloatTensorLike, Callable, Dict] = 1e-4,\n",
    "            beta_1: FloatTensorLike = 0.9,\n",
    "            beta_2: FloatTensorLike = 0.999,\n",
    "            epsilon: FloatTensorLike = 1e-12,\n",
    "            warmup_steps: int = 10000,\n",
    "            name: str = \"Radabelief\",\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(name, **kwargs)\n",
    "\n",
    "        self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate))\n",
    "        self._set_hyper(\"beta_1\", beta_1)\n",
    "        self._set_hyper(\"beta_2\", beta_2)\n",
    "        self._set_hyper(\"warmup_steps\", warmup_steps)\n",
    "        self.epsilon = epsilon or tf.keras.backend.epsilon()\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"m\")\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"v\")\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        params = self.weights\n",
    "        num_vars = int((len(params) - 1) / 2)\n",
    "        if len(weights) == 3 * num_vars + 1:\n",
    "            weights = weights[: len(params)]\n",
    "        super().set_weights(weights)\n",
    "\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "\n",
    "        # *current* learn rate of optimizer (changed by ReduceLROnPlateau for example)\n",
    "        lr_t = self.lr\n",
    "\n",
    "        # get previous moments\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        v = self.get_slot(var, \"v\")\n",
    "\n",
    "        # static hyperparameters of optimizer\n",
    "        lr_0 = self._get_hyper(\"learning_rate\", var_dtype)\n",
    "        warmup_steps = self._get_hyper(\"warmup_steps\", var_dtype)\n",
    "        beta_1 = self._get_hyper(\"beta_1\", var_dtype)\n",
    "        beta_2 = self._get_hyper(\"beta_2\", var_dtype)\n",
    "\n",
    "        # smaller epsilon == more bias == more like SGD\n",
    "        # larger epsilon == more adaptive, potential for large LR difference between variables\n",
    "        epsilon_t = tf.convert_to_tensor(self.epsilon, var_dtype)\n",
    "\n",
    "        # current step of optimizer\n",
    "        local_step = tf.cast(self.iterations + 1, var_dtype)\n",
    "\n",
    "        # use linearly scaled lr from 0 to initial LR while steps under 'warmup_steps'\n",
    "        lr = tf.where(\n",
    "            local_step <= warmup_steps,\n",
    "            (local_step / warmup_steps) * lr_0,\n",
    "            lr_t,\n",
    "        )\n",
    "\n",
    "        # calculate first moment of gradient (momemtum)\n",
    "        m_t = m.assign(\n",
    "            beta_1 * m + (1.0 - beta_1) * grad,\n",
    "            use_locking=self._use_locking\n",
    "        )\n",
    "\n",
    "        # calculate second moment of gradient (RMSprop)\n",
    "        # use 'tf.square(grad - m_t)' for Adabelief instead of 'tf.square(grad)'\n",
    "        v_t = v.assign(\n",
    "            beta_2 * v + (1.0 - beta_2) * tf.square(grad - m_t),\n",
    "            use_locking=self._use_locking,\n",
    "        )\n",
    "\n",
    "        # correct bias (mostly affects initial steps)\n",
    "        m_corr_t = m_t / (1.0 - tf.pow(beta_1, local_step))\n",
    "        v_corr_t = v_t / (1.0 - tf.pow(beta_2, local_step))\n",
    "\n",
    "        # calculate step\n",
    "        var_t = m_corr_t / (tf.sqrt(v_corr_t) + epsilon_t)\n",
    "\n",
    "        # apply learn rate\n",
    "        var_update = var.assign_sub(lr * var_t,\n",
    "                                    use_locking=self._use_locking)\n",
    "\n",
    "        updates = [var_update, m_t, v_t]\n",
    "        return tf.group(*updates)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "                \"beta_1\": self._serialize_hyperparameter(\"beta_1\"),\n",
    "                \"beta_2\": self._serialize_hyperparameter(\"beta_2\"),\n",
    "                \"epsilon\": self.epsilon,\n",
    "                \"warmup_steps\": self._serialize_hyperparameter(\"warmup_steps\"),\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}